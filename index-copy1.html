<!doctype html>
<html>
<head>
<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>
<title>index</title><link href='https://fonts.googleapis.com/css?family=Open+Sans:400italic,700italic,700,400&subset=latin,latin-ext' rel='stylesheet' type='text/css' /><style type='text/css'>html {overflow-x: initial !important;}:root { --bg-color:#ffffff; --text-color:#333333; --select-text-bg-color:#B5D6FC; --select-text-font-color:auto; }
html { font-size: 14px; background-color: var(--bg-color); color: var(--text-color); font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; -webkit-font-smoothing: antialiased; }
body { margin: 0px; padding: 0px; height: auto; bottom: 0px; top: 0px; left: 0px; right: 0px; font-size: 1rem; line-height: 1.42857; overflow-x: hidden; background: inherit; }
a.url { word-break: break-all; }
a:active, a:hover { outline: 0px; }
.in-text-selection, ::selection { text-shadow: none; background: var(--select-text-bg-color); color: var(--select-text-font-color); }
#write { margin: 0px auto; height: auto; width: inherit; word-break: normal; word-wrap: break-word; position: relative; padding-bottom: 70px; white-space: pre-wrap; overflow-x: visible; }
.first-line-indent #write p .md-line { text-indent: 0px; }
.first-line-indent #write li, .first-line-indent #write p, .first-line-indent #write p .md-line:first-child { text-indent: 2em; }
.for-image #write { padding-left: 8px; padding-right: 8px; }
body.typora-export { padding-left: 30px; padding-right: 30px; }
@media screen and (max-width: 500px) {
  body.typora-export { padding-left: 0px; padding-right: 0px; }
  .CodeMirror-sizer { margin-left: 0px !important; }
  .CodeMirror-gutters { display: none !important; }
}
#write > blockquote:first-child, #write > div:first-child, #write > figure:first-child, #write > ol:first-child, #write > p:first-child, #write > pre:first-child, #write > ul:first-child { margin-top: 30px; }
#write li > figure:first-child { margin-top: -20px; }
#write ol, #write ul { position: relative; }
img { max-width: 100%; vertical-align: middle; }
button, input, select, textarea { color: inherit; font-style: inherit; font-variant: inherit; font-weight: inherit; font-stretch: inherit; font-size: inherit; line-height: inherit; font-family: inherit; }
input[type="checkbox"], input[type="radio"] { line-height: normal; padding: 0px; }
*, ::after, ::before { box-sizing: border-box; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p, #write pre { width: inherit; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p { position: relative; }
h1, h2, h3, h4, h5, h6 { break-after: avoid-page; break-inside: avoid; orphans: 2; }
p { orphans: 4; }
h1 { font-size: 2rem; }
h2 { font-size: 1.8rem; }
h3 { font-size: 1.6rem; }
h4 { font-size: 1.4rem; }
h5 { font-size: 1.2rem; }
h6 { font-size: 1rem; }
h1, h2, h3, h4, h5, h6 { margin-top: 1rem; margin-bottom: 1rem; }
p { -webkit-margin-before: 1rem; -webkit-margin-after: 1rem; -webkit-margin-start: 0px; -webkit-margin-end: 0px; }
.mathjax-block { margin-top: 0px; margin-bottom: 0px; -webkit-margin-before: 0px; -webkit-margin-after: 0px; }
.hidden { display: none; }
.md-blockmeta { color: rgb(204, 204, 204); font-weight: 700; font-style: italic; }
a { cursor: pointer; }
sup.md-footnote { padding: 2px 4px; background-color: rgba(238, 238, 238, 0.7); color: rgb(85, 85, 85); border-radius: 4px; cursor: pointer; }
sup.md-footnote a, sup.md-footnote a:hover { color: inherit; text-transform: inherit; text-decoration: inherit; }
#write input[type="checkbox"] { cursor: pointer; width: inherit; height: inherit; }
figure { overflow-x: auto; margin: 1.2em 0px; max-width: calc(100% + 16px); padding: 0px; }
figure > table { margin: 0px !important; }
tr { break-inside: avoid; break-after: auto; }
thead { display: table-header-group; }
table { border-collapse: collapse; border-spacing: 0px; width: 100%; overflow: auto; break-inside: auto; text-align: left; }
table.md-table td { min-width: 80px; }
.CodeMirror-gutters { border-right: 0px; background-color: inherit; }
.CodeMirror { text-align: left; }
.CodeMirror-placeholder { opacity: 0.3; }
.CodeMirror pre { padding: 0px 4px; }
.CodeMirror-lines { padding: 0px; }
div.hr:focus { cursor: none; }
pre { white-space: pre-wrap; }
pre.contain-cm { white-space: normal; }
.CodeMirror-gutters { margin-right: 4px; }
.md-fences { font-size: 0.9rem; display: block; break-inside: avoid; text-align: left; overflow: visible; white-space: pre; background: inherit; position: relative !important; }
.md-diagram-panel { width: 100%; margin-top: 10px; text-align: center; padding-top: 0px; padding-bottom: 8px; overflow-x: auto; }
.md-fences.mock-cm { white-space: pre-wrap; }
.show-fences-line-number .md-fences { padding-left: 0px; }
.show-fences-line-number .md-fences.mock-cm { padding-left: 40px; }
.CodeMirror-line { break-inside: avoid; }
.footnotes { opacity: 0.8; font-size: 0.9rem; margin-top: 1em; margin-bottom: 1em; }
.footnotes + .footnotes { margin-top: 0px; }
.md-reset { margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: top; background: 0px 0px; text-decoration: none; text-shadow: none; float: none; position: static; width: auto; height: auto; white-space: nowrap; cursor: inherit; -webkit-tap-highlight-color: transparent; line-height: normal; font-weight: 400; text-align: left; box-sizing: content-box; direction: ltr; }
li div { padding-top: 0px; }
blockquote { margin: 1rem 0px; }
li .mathjax-block, li p { margin: 0.5rem 0px; }
li { margin: 0px; position: relative; }
blockquote > :last-child { margin-bottom: 0px; }
blockquote > :first-child, li > :first-child { margin-top: 0px; }
.footnotes-area { color: rgb(136, 136, 136); margin-top: 0.714rem; padding-bottom: 0.143rem; white-space: normal; }
.footnote-line { white-space: pre-wrap; }
@media print {
  body, html { border: 1px solid transparent; height: 99%; break-after: avoid; break-before: avoid; }
  #write { margin-top: 0px; border-color: transparent !important; }
  .typora-export * { -webkit-print-color-adjust: exact; }
  html.blink-to-pdf { font-size: 13px; }
  .typora-export #write { padding-left: 1cm; padding-right: 1cm; padding-bottom: 0px; break-after: avoid; }
  .typora-export #write::after { height: 0px; }
  @page { margin: 20mm 0px; }
}
.footnote-line { margin-top: 0.714em; font-size: 0.7em; }
a img, img a { cursor: pointer; }
pre.md-meta-block { font-size: 0.8rem; min-height: 0.8rem; white-space: pre-wrap; background: rgb(204, 204, 204); display: block; overflow-x: hidden; }
p > img:only-child { display: block; margin: auto; }
.md-line > .md-image:only-child, p > .md-image:only-child { display: inline-block; width: 100%; text-align: center; }
#write .MathJax_Display { margin: 0.8em 0px 0px; }
.mathjax-block { white-space: pre; overflow: hidden; width: 100%; }
p + .mathjax-block { margin-top: -1.143rem; }
.mathjax-block:not(:empty)::after { display: none; }
[contenteditable="true"]:active, [contenteditable="true"]:focus { outline: 0px; box-shadow: none; }
.md-task-list-item { position: relative; list-style-type: none; }
.task-list-item.md-task-list-item { padding-left: 0px; }
.md-task-list-item > input { position: absolute; top: 0px; left: 0px; margin-left: -1.2em; margin-top: calc(1em - 10px); }
.math { font-size: 1rem; }
.md-toc { min-height: 3.58rem; position: relative; font-size: 0.9rem; border-radius: 10px; }
.md-toc-content { position: relative; margin-left: 0px; }
.md-toc-content::after, .md-toc::after { display: none; }
.md-toc-item { display: block; color: rgb(65, 131, 196); }
.md-toc-item a { text-decoration: none; }
.md-toc-inner:hover { }
.md-toc-inner { display: inline-block; cursor: pointer; }
.md-toc-h1 .md-toc-inner { margin-left: 0px; font-weight: 700; }
.md-toc-h2 .md-toc-inner { margin-left: 2em; }
.md-toc-h3 .md-toc-inner { margin-left: 4em; }
.md-toc-h4 .md-toc-inner { margin-left: 6em; }
.md-toc-h5 .md-toc-inner { margin-left: 8em; }
.md-toc-h6 .md-toc-inner { margin-left: 10em; }
@media screen and (max-width: 48em) {
  .md-toc-h3 .md-toc-inner { margin-left: 3.5em; }
  .md-toc-h4 .md-toc-inner { margin-left: 5em; }
  .md-toc-h5 .md-toc-inner { margin-left: 6.5em; }
  .md-toc-h6 .md-toc-inner { margin-left: 8em; }
}
a.md-toc-inner { font-size: inherit; font-style: inherit; font-weight: inherit; line-height: inherit; }
.footnote-line a:not(.reversefootnote) { color: inherit; }
.md-attr { display: none; }
.md-fn-count::after { content: "."; }
code, pre, tt { font-family: var(--monospace); }
.md-comment { color: rgb(162, 127, 3); opacity: 0.8; font-family: var(--monospace); }
code { text-align: left; }
a.md-print-anchor { border-width: initial !important; border-style: none !important; border-color: initial !important; display: inline-block !important; position: absolute !important; width: 1px !important; right: 0px !important; outline: 0px !important; background: 0px 0px !important; text-decoration: initial !important; text-shadow: initial !important; }
.md-inline-math .MathJax_SVG .noError { display: none !important; }
.mathjax-block .MathJax_SVG_Display { text-align: center; margin: 1em 0px; position: relative; text-indent: 0px; max-width: none; max-height: none; min-height: 0px; min-width: 100%; width: auto; display: block !important; }
.MathJax_SVG_Display, .md-inline-math .MathJax_SVG_Display { width: auto; margin: inherit; display: inline-block !important; }
.MathJax_SVG .MJX-monospace { font-family: monospace; }
.MathJax_SVG .MJX-sans-serif { font-family: sans-serif; }
.MathJax_SVG { display: inline; font-style: normal; font-weight: 400; line-height: normal; zoom: 90%; text-indent: 0px; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; padding: 0px; margin: 0px; }
.MathJax_SVG * { transition: none; }
.os-windows.monocolor-emoji .md-emoji { font-family: "Segoe UI Symbol", sans-serif; }
.md-diagram-panel > svg { max-width: 100%; }
[lang="flow"] svg, [lang="mermaid"] svg { max-width: 100%; }
[lang="mermaid"] .node text { font-size: 1rem; }
table tr th { border-bottom: 0px; }


:root { --side-bar-bg-color: #fafafa; --control-text-color: #777; }
@font-face { font-family: "Open Sans"; font-style: normal; font-weight: normal; src: local("Open Sans Regular"), url("./github/400.woff") format("woff"); }
@font-face { font-family: "Open Sans"; font-style: italic; font-weight: normal; src: local("Open Sans Italic"), url("./github/400i.woff") format("woff"); }
@font-face { font-family: "Open Sans"; font-style: normal; font-weight: bold; src: local("Open Sans Bold"), url("./github/700.woff") format("woff"); }
@font-face { font-family: "Open Sans"; font-style: italic; font-weight: bold; src: local("Open Sans Bold Italic"), url("./github/700i.woff") format("woff"); }
html { font-size: 16px; }
body { font-family: "Open Sans", "Clear Sans", "Helvetica Neue", Helvetica, Arial, sans-serif; color: rgb(51, 51, 51); line-height: 1.6; }
#write { max-width: 860px; margin: 0px auto; padding: 20px 30px 100px; }
#write > ul:first-child, #write > ol:first-child { margin-top: 30px; }
body > :first-child { margin-top: 0px !important; }
body > :last-child { margin-bottom: 0px !important; }
a { color: rgb(65, 131, 196); }
h1, h2, h3, h4, h5, h6 { position: relative; margin-top: 1rem; margin-bottom: 1rem; font-weight: bold; line-height: 1.4; cursor: text; }
h1:hover a.anchor, h2:hover a.anchor, h3:hover a.anchor, h4:hover a.anchor, h5:hover a.anchor, h6:hover a.anchor { text-decoration: none; }
h1 tt, h1 code { font-size: inherit; }
h2 tt, h2 code { font-size: inherit; }
h3 tt, h3 code { font-size: inherit; }
h4 tt, h4 code { font-size: inherit; }
h5 tt, h5 code { font-size: inherit; }
h6 tt, h6 code { font-size: inherit; }
h1 { padding-bottom: 0.3em; font-size: 2.25em; line-height: 1.2; border-bottom: 1px solid rgb(238, 238, 238); }
h2 { padding-bottom: 0.3em; font-size: 1.75em; line-height: 1.225; border-bottom: 1px solid rgb(238, 238, 238); }
h3 { font-size: 1.5em; line-height: 1.43; }
h4 { font-size: 1.25em; }
h5 { font-size: 1em; }
h6 { font-size: 1em; color: rgb(119, 119, 119); }
p, blockquote, ul, ol, dl, table { margin: 0.8em 0px; }
li > ol, li > ul { margin: 0px; }
hr { height: 2px; padding: 0px; margin: 16px 0px; background-color: rgb(231, 231, 231); border: 0px none; overflow: hidden; box-sizing: content-box; }
body > h2:first-child { margin-top: 0px; padding-top: 0px; }
body > h1:first-child { margin-top: 0px; padding-top: 0px; }
body > h1:first-child + h2 { margin-top: 0px; padding-top: 0px; }
body > h3:first-child, body > h4:first-child, body > h5:first-child, body > h6:first-child { margin-top: 0px; padding-top: 0px; }
a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 { margin-top: 0px; padding-top: 0px; }
h1 p, h2 p, h3 p, h4 p, h5 p, h6 p { margin-top: 0px; }
li p.first { display: inline-block; }
ul, ol { padding-left: 30px; }
ul:first-child, ol:first-child { margin-top: 0px; }
ul:last-child, ol:last-child { margin-bottom: 0px; }
blockquote { border-left: 4px solid rgb(223, 226, 229); padding: 0px 15px; color: rgb(119, 119, 119); }
blockquote blockquote { padding-right: 0px; }
table { padding: 0px; word-break: initial; }
table tr { border-top: 1px solid rgb(223, 226, 229); margin: 0px; padding: 0px; }
table tr:nth-child(2n), thead { background-color: rgb(248, 248, 248); }
table tr th { font-weight: bold; border-width: 1px 1px 0px; border-top-style: solid; border-right-style: solid; border-left-style: solid; border-top-color: rgb(223, 226, 229); border-right-color: rgb(223, 226, 229); border-left-color: rgb(223, 226, 229); border-image: initial; border-bottom-style: initial; border-bottom-color: initial; text-align: left; margin: 0px; padding: 6px 13px; }
table tr td { border: 1px solid rgb(223, 226, 229); text-align: left; margin: 0px; padding: 6px 13px; }
table tr th:first-child, table tr td:first-child { margin-top: 0px; }
table tr th:last-child, table tr td:last-child { margin-bottom: 0px; }
.CodeMirror-gutters { border-right: 1px solid rgb(221, 221, 221); }
.md-fences, code, tt { border: 1px solid rgb(223, 226, 229); background-color: rgb(248, 248, 248); border-radius: 3px; font-family: Consolas, "Liberation Mono", Courier, monospace; padding: 2px 4px 0px; font-size: 0.9em; }
.md-fences { margin-bottom: 15px; margin-top: 15px; padding: 8px 1em 6px; }
.md-task-list-item > input { margin-left: -1.3em; }
@media screen and (min-width: 914px) {
}
@media print {
  html { font-size: 13px; }
  table, pre { break-inside: avoid; }
  pre { word-wrap: break-word; }
}
.md-fences { background-color: rgb(248, 248, 248); }
#write pre.md-meta-block { padding: 1rem; font-size: 85%; line-height: 1.45; background-color: rgb(247, 247, 247); border: 0px; border-radius: 3px; color: rgb(119, 119, 119); margin-top: 0px !important; }
.mathjax-block > .code-tooltip { bottom: 0.375rem; }
#write > h3.md-focus::before { left: -1.5625rem; top: 0.375rem; }
#write > h4.md-focus::before { left: -1.5625rem; top: 0.285714rem; }
#write > h5.md-focus::before { left: -1.5625rem; top: 0.285714rem; }
#write > h6.md-focus::before { left: -1.5625rem; top: 0.285714rem; }
.md-image > .md-meta { border-radius: 3px; font-family: Consolas, "Liberation Mono", Courier, monospace; padding: 2px 0px 0px 4px; font-size: 0.9em; color: inherit; }
.md-tag { color: inherit; }
.md-toc { margin-top: 20px; padding-bottom: 20px; }
.sidebar-tabs { border-bottom: none; }
#typora-quick-open { border: 1px solid rgb(221, 221, 221); background-color: rgb(248, 248, 248); }
#typora-quick-open-item { background-color: rgb(250, 250, 250); border-color: rgb(254, 254, 254) rgb(229, 229, 229) rgb(229, 229, 229) rgb(238, 238, 238); border-style: solid; border-width: 1px; }
#md-notification::before { top: 10px; }
.on-focus-mode blockquote { border-left-color: rgba(85, 85, 85, 0.12); }
header, .context-menu, .megamenu-content, footer { font-family: "Segoe UI", Arial, sans-serif; }
.file-node-content:hover .file-node-icon, .file-node-content:hover .file-node-open-state { visibility: visible; }
.mac-seamless-mode #typora-sidebar { background-color: var(--side-bar-bg-color); }
.md-lang { color: rgb(180, 101, 77); }
.html-for-mac .context-menu { --item-hover-bg-color: #E6F0FE; }





 .typora-export p, .typora-export .footnote-line {white-space: normal;} 
</style>
</head>
<body class='typora-export os-windows' >
<div  id='write'  class = 'is-node'><p> Kai Yi, SE senior student focused on Computer vision &amp; Machine Learning      <br/></p><p>About Publications Projects <a href='http://nbviewer.jupyter.org/github/yihui-he/cv-yihui/blob/master/cv_yihui.pdf'>CV</a></p><p><br/></p><ul><li><p>About</p></li><li><p>Publications</p></li><li><p>Projects</p></li><li><p><a href='http://nbviewer.jupyter.org/github/yihui-he/cv-yihui/blob/master/cv_yihui.pdf'>CV</a></p></li><li><ul><li>*</li></ul><p>&nbsp;</p></li></ul><p>williamyi96 AT gmail.com<br/></p><p><br/></p><h3><a name='header-n31' class='md-header-anchor '></a>Kai Yi (易凯)</h3><h5><a name='header-n32' class='md-header-anchor '></a>williamyi96 AT gmail.com</h5><p> <a href='http://daggerfs.com/index.html#'><img src='assets_files/me.png' alt='' referrerPolicy='no-referrer' /></a> </p><p>I&#39;m a senior student from Xi&#39;an Jiaotong University, major in software engineering. Now pursuing my PHD. My interest focus on cognition-based artificial intelligence, machine learning, computer vision and computational psychology.</p><p>I&#39;ve interned at <a href='http://www.aiar.xjtu.edu.cn/'>IAIR</a> (Institute of Artificial Intelligence and Robotics) supervised by Prof. <a href='http://www.aiar.xjtu.edu.cn/info/1015/1071.htm'>Nanning Zheng</a> and <a href='http://gr.xjtu.edu.cn/web/yufeng/lab'>Moral Psycholoy Lab </a> supervised by Professor Liang Zhao and <a href='http://gr.xjtu.edu.cn/web/yufeng/english-version'>Feng Yu</a>. Also, I&#39;m now an intern at <a href='http://www.aafie.org/'>AAFIE</a>.</p><p>I&#39;ve served as a reviewer for ITSC18(The 21st IEEE International Conference on Intelligent Transportation Systems) .</p><p><strong>Planning to pursue a Ph.D. If you&#39;re interested, don’t hesitate to contact me.</strong> Here&#39;s my <a href='http://nbviewer.jupyter.org/github/yihui-he/cv-yihui/blob/master/cv_yihui.pdf'>CV</a>.</p><h3><a name='header-n43' class='md-header-anchor '></a>Publications</h3><p>Link to [[Google Scholar]](<a href='https://scholar.google.com/citations?user=r08j39wAAAAJ' target='_blank' class='url'>https://scholar.google.com/citations?user=r08j39wAAAAJ</a>)</p><p> <img src='./assets_files/mobile.png' alt='' referrerPolicy='no-referrer' /></p><p><strong>AMC: AutoML for Model Compression and Acceleration on Mobile Devices</strong><br/>
<strong>Yihui He</strong>, Ji Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, <a href='https://stanford.edu/~songhan/'>Song Han</a>, <strong>ECCV 2018</strong> [[PDF]](<a href='http://openaccess.thecvf.com/content_ECCV_2018/html/Yihui_He_AMC_Automated_Model_ECCV_2018_paper.html' target='_blank' class='url'>http://openaccess.thecvf.com/content_ECCV_2018/html/Yihui_He_AMC_Automated_Model_ECCV_2018_paper.html</a>) [[arXiv]](<a href='https://arxiv.org/abs/1802.03494' target='_blank' class='url'>https://arxiv.org/abs/1802.03494</a>)</p><p>In this paper, we propose AMC: AutoML for Model Compression that leverage reinforcement learning to efficiently sample the design space and greatly improve the model compression quality.</p><p> <img src='./assets_files/ill-1.png' alt='' referrerPolicy='no-referrer' /></p><p><strong>Channel pruning for accelerating very deep neural networks</strong><br/>
<strong>Yihui He</strong>, Xiangyu Zhang, <a href='http://jiansun.org/'>Jian Sun</a>, <strong>ICCV 2017</strong> [[PDF]](<a href='http://openaccess.thecvf.com/content_iccv_2017/html/He_Channel_Pruning_for_ICCV_2017_paper.html' target='_blank' class='url'>http://openaccess.thecvf.com/content_iccv_2017/html/He_Channel_Pruning_for_ICCV_2017_paper.html</a>) [[arXiv]](<a href='https://arxiv.org/abs/1707.06168' target='_blank' class='url'>https://arxiv.org/abs/1707.06168</a>) [[Poster]](<a href='https://raw.githubusercontent.com/yihui-he/images/master/38722_He_0600.png' target='_blank' class='url'>https://raw.githubusercontent.com/yihui-he/images/master/38722_He_0600.png</a>) [[Slides]](<a href='http://nbviewer.jupyter.org/github/yihui-he/images/blob/master/channel-pruning-methods.pdf' target='_blank' class='url'>http://nbviewer.jupyter.org/github/yihui-he/images/blob/master/channel-pruning-methods.pdf</a>) [[Code]](<a href='https://github.com/yihui-he/channel-pruning' target='_blank' class='url'>https://github.com/yihui-he/channel-pruning</a>)</p><p>In this paper, we introduce a new channel pruning method to accelerate very deep convolutional neural networks.Given a trained CNN model, we propose an iterative two-step algorithm to effectively prune each layer, by a LASSO regression based channel selection and least square reconstruction. We further generalize this algorithm to multi-layer and multi-branch cases. Our method reduces the accumulated error and enhance the compatibility with various architectures. Our pruned VGG-16 achieves the state-of-the-art results by 5x speed-up along with only 0.3% increase of error. More importantly, our method is able to accelerate modern networks like ResNet, Xception and suffers only 1.4%, 1.0% accuracy loss under 2x speed-up respectively, which is significant.</p><p> <img src='./assets_files/vehicle.png' alt='' referrerPolicy='no-referrer' /></p><p><strong>Vehicle Traffic Driven Camera Placement for Better Metropolis Security Surveillance</strong><br/>
Xiaobo Ma<em>, <strong>Yihui He</strong></em>, Xiapu Luo, Jianfeng Li, Xiaohong Guan , <strong>IEEE Intelligent System</strong> [[PDF]](<a href='https://ieeexplore.ieee.org/abstract/document/8354911/' target='_blank' class='url'>https://ieeexplore.ieee.org/abstract/document/8354911/</a>) [[arXiv]](<a href='http://arxiv.org/abs/1705.08508' target='_blank' class='url'>http://arxiv.org/abs/1705.08508</a>) [[Code]](<a href='https://github.com/yihui-he/Vehicle-Traffic-Driven-Camera-Placement' target='_blank' class='url'>https://github.com/yihui-he/Vehicle-Traffic-Driven-Camera-Placement</a>)</p><p>Security surveillance is one of the most important issues in smart cities, especially in an era of terrorism. Deploying a number of (video) cameras is a common approach for surveillance information retrieval. Given the never-ending power offered by vehicles to a metropolis, exploiting vehicle traffic to design camera placement strategies could potentially facilitate physicalworld security surveillance. We take the first step towards exploring the linkage between vehicle traffic and camera placement in favor of physical-world security surveillance from a network perspective.</p><p> <img src='./assets_files/filterwise.png' alt='' referrerPolicy='no-referrer' /></p><p><strong>Pruning Very Deep Neural Network Channels for Efficient Inference</strong><br/>
<strong>Yihui He</strong>, Xiangyu Zhang, <a href='http://jiansun.org/'>Jian Sun</a> , <em>TPAMI, Major Revision</em></p><p>Channel Pruning is further expanded to Filterwise Pruning with rich experiements.</p><p> <img src='./assets_files/superres.png' alt='' referrerPolicy='no-referrer' /></p><p><strong>Single Image Super-resolution with a Parameter Economic Residual-like Convolutional Neural Network</strong><br/>
Yudong Liang, Ze Yang, Kai Zhang, <strong>Yihui He</strong>, Jinjun Wang, Nanning Zheng, <em>TMM in submission</em> [[arXiv]](<a href='https://arxiv.org/abs/1703.08173' target='_blank' class='url'>https://arxiv.org/abs/1703.08173</a>)</p><p>This paper aims to extend the merits of residual network, such as skip connection induced fast training, for a typical low-level vision problem, i.e., single image super-resolution. In general, the two main challenges of existing deep CNN for supper-resolution lie in the gradient exploding/vanishing problem and large amount of parameters or computational cost as CNN goes deeper. Correspondingly, the skip connections or identity mapping shortcuts are utilized to avoid gradient exploding/vanishing problem. To tackle with the second problem, a parameter economic CNN architecture which has carefully designed width, depth and skip connections was proposed. Different residual-like architectures for image superresolution has also been compared. Experimental results have demonstrated that the proposed CNN model can not only achieve state-of-the-art PSNR and SSIM results for single image super-resolution but also produce visually pleasant results.</p><h3><a name='header-n81' class='md-header-anchor '></a>Projects</h3><p>Link to my [[github public projects]](<a href='https://github.com/yihui-he' target='_blank' class='url'>https://github.com/yihui-he</a>)</p><p> <img src='https://raw.githubusercontent.com/yihui-he/TSP-report/master/02132.png' alt='' referrerPolicy='no-referrer' /></p><p><strong>An Empirical Analysis of Approximation Algorithms for the Euclidean Traveling Salesman Problem</strong> [[arXiv]](<a href='https://arxiv.org/abs/1705.09058' target='_blank' class='url'>https://arxiv.org/abs/1705.09058</a>) [[Code]](<a href='https://github.com/yihui-he/TSP' target='_blank' class='url'>https://github.com/yihui-he/TSP</a>)</p><p>we perform an evaluation and analysis of cornerstone algorithms for the metric TSP. We evaluate greedy, 2-opt, and genetic algorithms. We use several datasets as input for the algorithms including a small dataset, a medium-sized dataset representing cities in the United States, and a synthetic dataset consisting of 200 cities to test algorithm scalability.</p><p> <img src='https://raw.githubusercontent.com/yihui-he/Depth-estimation-with-neural-network/master/presentation/stereo.png?token=AJkBS_A-YWaMd9vcgEQuaXQWe9wmjtTBks5XWM07wA%3D%3D' alt='' referrerPolicy='no-referrer' /></p><p><strong>Estimated Depth Map Helps Image Classification</strong> [[arXiv]](<a href='https://arxiv.org/abs/1709.07077' target='_blank' class='url'>https://arxiv.org/abs/1709.07077</a>) [[Code]](<a href='https://github.com/yihui-he/Estimated-Depth-Map-Helps-Image-Classification' target='_blank' class='url'>https://github.com/yihui-he/Estimated-Depth-Map-Helps-Image-Classification</a>) [[Slides]](<a href='http://nbviewer.jupyter.org/github/yihui-he/Estimated-Depth-Map-Helps-Image-Classification/blob/master/presentation/depth.pdf' target='_blank' class='url'>http://nbviewer.jupyter.org/github/yihui-he/Estimated-Depth-Map-Helps-Image-Classification/blob/master/presentation/depth.pdf</a>)</p><p>We consider image classification with estimated depth. This problem falls into the domain of transfer learning, since we are using a model trained on a set of depth images to generate depth maps (additional features) for use in another classification problem using another disjoint set of images. It&#39;s challenging as no direct depth information is provided. Though depth estimation has been well studied, none have attempted to aid image classification with estimated depth. Therefore, we present a way of transferring domain knowledge on depth estimation to a separate image classification task over a disjoint set of train, and test data. We build a RGBD dataset based on RGB dataset and do image classification on it. Then evaluation the performance of neural networks on the RGBD dataset compared to the RGB dataset. From our experiments, the benefit is significant with shallow and deep networks. It improves ResNet-20 by 0.55% and ResNet-56 by 0.53%.</p><p> <img src='https://www.kaggle.io/svf/310043/4567203286d71c8fd31cf12668a3ceac/__results___files/__results___5_0.png' alt='' referrerPolicy='no-referrer' /></p><p><strong>Medical Image Segmentation: A survey</strong> [[Code]](<a href='https://github.com/yihui-he/u-net' target='_blank' class='url'>https://github.com/yihui-he/u-net</a>) [[Summary]](<a href='https://github.com/yihui-he/medical-image-segmentation-a-survey/blob/master/README.md' target='_blank' class='url'>https://github.com/yihui-he/medical-image-segmentation-a-survey/blob/master/README.md</a>)</p><p>I evaluate DeepMask, Deeplab and MNC for medical image segmentation. I ranked <strong>19%</strong> on <a href='https://www.kaggle.com/c/ultrasound-nerve-segmentation/leaderboard/private'>Kaggle Ultrasound Nerve Segmentation</a></p><p> <img src='https://raw.githubusercontent.com/yihui-he/deep-learning-guide/master/presentation/resnet.png' alt='' referrerPolicy='no-referrer' /></p><p><strong>residual neural network with tensorflow on CIFAR-100</strong> [[PDF]](<a href='http://nbviewer.jupyter.org/github/yihui-he/ResNet-tensorflow/blob/master/report/mp2_Yihui%20He.pdf' target='_blank' class='url'>http://nbviewer.jupyter.org/github/yihui-he/ResNet-tensorflow/blob/master/report/mp2_Yihui%20He.pdf</a>) [[Code]](<a href='https://github.com/yihui-he/ResNet-tensorflow' target='_blank' class='url'>https://github.com/yihui-he/ResNet-tensorflow</a>) [[Slides]](<a href='http://nbviewer.jupyter.org/github/yihui-he/deep-learning-guide/blob/master/presentation/mp12.pdf' target='_blank' class='url'>http://nbviewer.jupyter.org/github/yihui-he/deep-learning-guide/blob/master/presentation/mp12.pdf</a>)</p><p>I reimplement 6/13/20 layers RNN in tensorflow from scratch, and test it’s result on CIFAR10/100.</p><p> <img src='https://raw.githubusercontent.com/yihui-he/deep-learning-guide/master/presentation/kmeans.jpg' alt='' referrerPolicy='no-referrer' /></p><p><strong>Single Layer neural network with PCAwhitening Kmeans</strong> [[PDF]](<a href='http://nbviewer.jupyter.org/github/yihui-he/An-Analysis-of-Single-Layer-Networks-in-Unsupervised-Feature-Learning/blob/master/report/mp1_Yihui%20He.pdf' target='_blank' class='url'>http://nbviewer.jupyter.org/github/yihui-he/An-Analysis-of-Single-Layer-Networks-in-Unsupervised-Feature-Learning/blob/master/report/mp1_Yihui%20He.pdf</a>) [[Code]](<a href='https://github.com/yihui-he/An-Analysis-of-Single-Layer-Networks-in-Unsupervised-Feature-Learning' target='_blank' class='url'>https://github.com/yihui-he/An-Analysis-of-Single-Layer-Networks-in-Unsupervised-Feature-Learning</a>) [[Slides]](<a href='http://nbviewer.jupyter.org/github/yihui-he/deep-learning-guide/blob/master/presentation/mp12.pdf' target='_blank' class='url'>http://nbviewer.jupyter.org/github/yihui-he/deep-learning-guide/blob/master/presentation/mp12.pdf</a>)</p><p>We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks. We also released the software and pre-trained network to do large-scale image classification.</p><p> <img src='https://raw.githubusercontent.com/yihui-he/my-Notes/master/person.jpg.png' alt='' referrerPolicy='no-referrer' /></p><p><strong>Objects Detection with YOLO on Artwork Dataset</strong> [[PDF]](<a href='http://nbviewer.jupyter.org/github/yihui-he/Objects-Detection-with-YOLO-on-Artwork-Dataset/blob/master/Report_Yihui.pdf' target='_blank' class='url'>http://nbviewer.jupyter.org/github/yihui-he/Objects-Detection-with-YOLO-on-Artwork-Dataset/blob/master/Report_Yihui.pdf</a>) [[Code]](<a href='https://github.com/yihui-he/Objects-Detection-with-YOLO-on-Artwork-Dataset' target='_blank' class='url'>https://github.com/yihui-he/Objects-Detection-with-YOLO-on-Artwork-Dataset</a>)</p><p>I design a small object detection network, which is simplified from YOLO(You Only Look Once) network. It&#39;s trained on PASCAL VOC. I evaluate it on an artwork dataset(Picasso dataset). With the best parameters, I got 40% precision and 35% recall.</p><p> <img src='./assets_files/shuttle.png' alt='' referrerPolicy='no-referrer' /></p><p><strong>shuttlecock detection and tracking</strong> [[Code]](<a href='https://github.com/yihui-he/Badminton-Robot' target='_blank' class='url'>https://github.com/yihui-he/Badminton-Robot</a>)</p><p>With guassian mixture model, I extract shuttlecock proposals. Then I use Partical filter to refine proposals. From multi view cameras, I employed structure from motion to predict its 3D location. Combined with Physics laws, landing location prediction accuracy is around 5 cm. (This system works on embeded linux with openCV)</p><hr /><h3><a name='header-n127' class='md-header-anchor '></a><strong>works</strong></h3><p><img src='https://raw.githubusercontent.com/yihui-he/lip-tracking-with-snake-active-contour-and-particle-filter/master/pic.png' alt='' referrerPolicy='no-referrer' /><img src='https://raw.githubusercontent.com/yihui-he/Edge-detection-with-zero-crossing/master/lena_1.bmp' alt='' referrerPolicy='no-referrer' /><img src='https://raw.githubusercontent.com/yihui-he/3D-reconstruction/master/result/selfff.png' alt='' referrerPolicy='no-referrer' /><img src='./assets_files/cs188.png' alt='' referrerPolicy='no-referrer' /><img src='https://raw.githubusercontent.com/yihui-he/my-Notes/master/person.jpg.png' alt='' referrerPolicy='no-referrer' /><img src='http://students.iitk.ac.in/robocon/images/sliders/master/bg-5.jpg' alt='' referrerPolicy='no-referrer' /><img src='./assets_files/vehicle.png' alt='' referrerPolicy='no-referrer' /><img src='https://raw.githubusercontent.com/yihui-he/Depth-estimation-with-neural-network/master/presentation/stereo.png?token=AJkBS_A-YWaMd9vcgEQuaXQWe9wmjtTBks5XWM07wA%3D%3D' alt='' referrerPolicy='no-referrer' /><img src='https://raw.githubusercontent.com/yihui-he/deep-learning-guide/master/presentation/resnet.png' alt='' referrerPolicy='no-referrer' /><img src='https://raw.githubusercontent.com/yihui-he/deep-learning-guide/master/presentation/kmeans.jpg' alt='' referrerPolicy='no-referrer' /><img src='./assets_files/shuttle.png' alt='' referrerPolicy='no-referrer' /><img src='https://raw.githubusercontent.com/yihui-he/my-Notes/master/gaoxin.jpg' alt='' referrerPolicy='no-referrer' /><img src='https://raw.githubusercontent.com/yihui-he/my-Notes/master/artwork.jpg' alt='' referrerPolicy='no-referrer' /><img src='https://raw.githubusercontent.com/yihui-he/my-Notes/master/bop.jpg' alt='' referrerPolicy='no-referrer' /><img src='./assets_files/ocsi.png' alt='' referrerPolicy='no-referrer' /></p><p><a href='https://github.com/yihui-he/panorama'><img src='https://github.com/yihui-he/panorama/blob/master/results/yellowstone5.jpg?raw=true' alt='' referrerPolicy='no-referrer' /></a></p><hr /><p>modified from <a href='http://daggerfs.com/'>? Yangqing Jia 2013</a></p></div>
</body>
</html>