<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-3232.ico?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.1616.ico?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta name="description" content="In this article, I will review the core concepts of deep reinforcement learning and introduce Deep Q-Network (DQN).  Basic IntroductionWe have witnessed the power of deep learning about solving high-c">
<meta property="og:type" content="article">
<meta property="og:title" content="A Glimpse of Deep Reinforcement Learning and DQN">
<meta property="og:url" content="williamyi96.github.io/2018/02/12/A-Glimpse-of-Deep-Reinforcement-Learning-and-DQN/index.html">
<meta property="og:site_name" content="Pursuing your dream">
<meta property="og:description" content="In this article, I will review the core concepts of deep reinforcement learning and introduce Deep Q-Network (DQN).  Basic IntroductionWe have witnessed the power of deep learning about solving high-c">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://img.blog.csdn.net/20170814092318915?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2lsbGlhbXlpOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20170814092307877?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2lsbGlhbXlpOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20170814092254146?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2lsbGlhbXlpOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20170814092242892?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2lsbGlhbXlpOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20170814092227003?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2lsbGlhbXlpOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20170814092208059?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2lsbGlhbXlpOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20170814092150986?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2lsbGlhbXlpOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:image" content="http://img.blog.csdn.net/20170814092135240?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2lsbGlhbXlpOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">
<meta property="og:updated_time" content="2018-02-12T09:39:12.544Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A Glimpse of Deep Reinforcement Learning and DQN">
<meta name="twitter:description" content="In this article, I will review the core concepts of deep reinforcement learning and introduce Deep Q-Network (DQN).  Basic IntroductionWe have witnessed the power of deep learning about solving high-c">
<meta name="twitter:image" content="http://img.blog.csdn.net/20170814092318915?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2lsbGlhbXlpOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="williamyi96.github.io/2018/02/12/A-Glimpse-of-Deep-Reinforcement-Learning-and-DQN/"/>





  <title>A Glimpse of Deep Reinforcement Learning and DQN | Pursuing your dream</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Pursuing your dream</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-schedule">
          <a href="/schedule/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br />
            
            Schedule
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="williamyi96.github.io/2018/02/12/A-Glimpse-of-Deep-Reinforcement-Learning-and-DQN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Kai Yi">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avater.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Pursuing your dream">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">A Glimpse of Deep Reinforcement Learning and DQN</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-02-12T16:51:38+08:00">
                2018-02-12
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>In this article, I will review the core concepts of deep reinforcement learning and introduce Deep Q-Network (DQN). </p>
<h1 id="Basic-Introduction"><a href="#Basic-Introduction" class="headerlink" title="Basic Introduction"></a>Basic Introduction</h1><p>We have witnessed the power of deep learning about solving high-computation problems and the strength of reinforcement learning at decision-making. Trying to combine those two methods is an obvious thing. This idea contributes to the appear of deep reinforcement learning. DQN, which was first proposed by OpenAI at the paper named “Play Atari with Deep Reinforcement Learning”, is a typical algorithm of deep reinforcement learning.</p>
<p>Meanwhile, the basic assumption behind reinforcement learning is that the agent can have a deeper understanding about the environment by interacting with it. And we can increase the ability to reach the goal after several iterations.</p>
<h1 id="MDPs"><a href="#MDPs" class="headerlink" title="MDPs"></a>MDPs</h1><p>Markov Decision Process satisfies the markov property, which is that the nest state is only determined by current state and current action instead of the history. </p>
<div align="left"><br><img src="http://img.blog.csdn.net/20170814092318915?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2lsbGlhbXlpOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"><br></div>

<p>MDPs consists of 5 components.</p>
<pre><code>S: the finite states sets
A: the finite actions sets
P: the state transition matrix
R: the reward function
gama: the discount factor
</code></pre><h1 id="Value-Function"><a href="#Value-Function" class="headerlink" title="Value Function"></a>Value Function</h1><p>We introduce the value function to measure the long-term reward and judge the policy in order to make better choice.</p>
<p>Value function can be normalized to bellman equation, the standard bellman equation can be solve by iteration.</p>
<p>Overall, there are three different ways to solve the value function:</p>
<ul>
<li>Dynamic  programming</li>
<li>Monte-Carlo method</li>
<li>Temporal differential method</li>
</ul>
<h2 id="Action-Value-function"><a href="#Action-Value-function" class="headerlink" title="Action-Value function"></a>Action-Value function</h2><p>We aim at analyzing the value of different actions towards current state, which brings about the action-value function.</p>
<p>We can regularize the action-state function into these form:</p>
<p><img src="http://img.blog.csdn.net/20170814092307877?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2lsbGlhbXlpOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<h2 id="Optimal-Value-function"><a href="#Optimal-Value-function" class="headerlink" title="Optimal Value function"></a>Optimal Value function</h2><p>We can normalize it into this form:</p>
<p><img src="http://img.blog.csdn.net/20170814092254146?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2lsbGlhbXlpOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<p>Then, after put it into action-value function, we can get :</p>
<p><img src="http://img.blog.csdn.net/20170814092242892?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2lsbGlhbXlpOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<h1 id="Iteration-based-on-Bellman-Equation"><a href="#Iteration-based-on-Bellman-Equation" class="headerlink" title="Iteration based on Bellman Equation:"></a>Iteration based on Bellman Equation:</h1><p>Widely used iteration methods designed to solve bellman equation can be categorized to policy iteration and value iteration.</p>
<h2 id="Policy-Iteration"><a href="#Policy-Iteration" class="headerlink" title="Policy Iteration"></a>Policy Iteration</h2><ol>
<li>Policy Evaluation: Estimate Vpi </li>
<li>Policy Improvement: Generate pi’ &gt;= pi</li>
</ol>
<p>pi can be converged to optimal proved theoretically.</p>
<h2 id="Value-Iteration"><a href="#Value-Iteration" class="headerlink" title="Value Iteration"></a>Value Iteration</h2><p>Value iteration is based on Bellman Optimal equation and convert it into iteration form.</p>
<h2 id="Diff-between-policy-iteration-and-value-iteration"><a href="#Diff-between-policy-iteration-and-value-iteration" class="headerlink" title="Diff. between policy iteration and value iteration"></a>Diff. between policy iteration and value iteration</h2><p>Policy iteration updates value by using the naive bellman equation. Meanwhile, the optimal value(vpi) is the optimal value at current policy, also called the estimation of a particular policy.</p>
<p>Value iteration update value by using Bellman Optimal Equation. Meanwhile, the optimal value(vpi) is the optimal value at current state.</p>
<p>Value iteration is more direct concerning getting an optimal value.</p>
<h1 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h1><p>The basic idea of Q learning is based on value iteration. We update Q value every value iteration, which means all the state and action. But due to the fact that we just get limited examples, Q learning puts forward a new way to update Q value:</p>
<p><img src="http://img.blog.csdn.net/20170814092227003?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2lsbGlhbXlpOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<p>We can demiss the influence of error like gradient descent method, which converges to the optimal Q value.</p>
<h2 id="Exploration-and-Exploitation"><a href="#Exploration-and-Exploitation" class="headerlink" title="Exploration and Exploitation"></a>Exploration and Exploitation</h2><ul>
<li>Off-policy: Need a policy to generate action, which means the selected policy is not the optimal policy.</li>
<li>Model-free: Not consider about the model(the detail information about the env), just care about seen env and reward.</li>
</ul>
<h3 id="Generate-action-by-policy"><a href="#Generate-action-by-policy" class="headerlink" title="Generate action by policy"></a>Generate action by policy</h3><ul>
<li>Exploration: Generate an action randomly. Benefitial to update Q value in order to get better policy.</li>
<li>Exploitation: Calculate an optimal action based on current Q value. Good to test whether an algorithm works while hard to update Q value.</li>
</ul>
<h3 id="ita-greedy-policy"><a href="#ita-greedy-policy" class="headerlink" title="ita-greedy policy"></a>ita-greedy policy</h3><p>We can combine exploration and exploitation by setting a fixed threshold ita, which method is called ita-greedy policy.</p>
<h1 id="DQN"><a href="#DQN" class="headerlink" title="DQN"></a>DQN</h1><h2 id="Dimemsion-Disaster"><a href="#Dimemsion-Disaster" class="headerlink" title="Dimemsion Disaster"></a>Dimemsion Disaster</h2><p>We store Q(s,a) in a table, which represents all the states and actions. But when we deal with image problems, the computation will be exponent that even can not be solved. So we need to reflect how to optimal the value function in another way.</p>
<p>Firstly, we introduced Value Function Approximation.</p>
<h2 id="Value-Function-Approximation"><a href="#Value-Function-Approximation" class="headerlink" title="Value Function Approximation"></a>Value Function Approximation</h2><p>In order to decrease dimension, we need to approximate the value function by another function. For example, we may use a linear function to approximate the value function, which just like this:</p>
<blockquote>
<p>Q(s,a) = f(s,a) = w1s + w2a + b</p>
</blockquote>
<p>Thus we get Q(s,a) approximates to f(s,a:w)</p>
<h2 id="Dimensionality-Reduction"><a href="#Dimensionality-Reduction" class="headerlink" title="Dimensionality Reduction"></a>Dimensionality Reduction</h2><p>As is often the case, the dimension of actions are obviously smaller than the dimension of states. In order to update value Q more efficiently, we need to reduce the dimension of states. just like this form:</p>
<blockquote>
<p>Q(s) approximates to f(s,w) where s is a vector: [Q(s,a1),…,Q(s,an)].</p>
</blockquote>
<h2 id="Training-Q-Network-by-DQN"><a href="#Training-Q-Network-by-DQN" class="headerlink" title="Training Q-Network by DQN"></a>Training Q-Network by DQN</h2><p>The typical deep neural network method is an optimal problem. The optimal target of neural network is the loss function, which is the bias between label and output. As the name suggests, the optimal of loss function is attempting to minimize the bias. We will need a lot of samples to train the parameters of neural network by policy gradient by back propagation.</p>
<p>Following the basic idea of neural network, we regard the target value Q as a label. And then trying to approximate value Q to target value Q.</p>
<p>Thus, the training loss function is:</p>
<p><img src="http://img.blog.csdn.net/20170814092208059?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2lsbGlhbXlpOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<h2 id="Naive-DQN"><a href="#Naive-DQN" class="headerlink" title="Naive DQN"></a>Naive DQN</h2><p><img src="http://img.blog.csdn.net/20170814092150986?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2lsbGlhbXlpOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<p>The basic idea of naive DQN is trying to train Q-Learning and SGD algorithm synchronously. Store all the sample and then randomly sampling, which is what we called experience replay. Learning by reflecting.</p>
<p>Trial for several times and then store all the data. Then do SGD by randomly sampling when getting a considerable number of data.</p>
<h1 id="Improved-DQN"><a href="#Improved-DQN" class="headerlink" title="Improved DQN"></a>Improved DQN</h1><p>There are several different methods used to improve the efficiency of DQN. Double DQN, Prioritied Replay and Dueling Network are three instinctive methods.</p>
<h2 id="Nature-DQN"><a href="#Nature-DQN" class="headerlink" title="Nature DQN"></a>Nature DQN</h2><p>Nature DQN means the methods mentioned on (Human Level Control …) by DeepMind. Nature DQN is also based on experience replay. The difference between it and naive DQN is that nature DQN introduced a Target Q network. Like this:</p>
<p><img src="http://img.blog.csdn.net/20170814092135240?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2lsbGlhbXlpOTY=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast" alt="这里写图片描述"></p>
<p>In order to decrease the relevance between target Q and current value Q, they designed a target Q network with updating delay, which means update the parameters after trained for a time.</p>
<p>The content of double DQN, prioritied replay and dueling network will be discussed later. This part remains to be seen after reading those papers.</p>
<p>And also, I will give a base summary of policy gradient method and A3C series about deep reinforcement learning.</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>[1] Mnih, Volodymyr, et al. “Playing atari with deep reinforcement learning.” arXiv preprint arXiv:1312.5602 (2013).</p>
<p>[2] Mnih, Volodymyr, et al. “Human-level control through deep reinforcement learning.” Nature 518.7540 (2015): 529-533.</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/02/12/Artificial-General-Intelligence-MIT/" rel="next" title="Artificial General Intelligence (MIT)">
                <i class="fa fa-chevron-left"></i> Artificial General Intelligence (MIT)
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avater.jpg"
                alt="Kai Yi" />
            
              <p class="site-author-name" itemprop="name">Kai Yi</p>
              <p class="site-description motion-element" itemprop="description">Researcher on cognitive science, computer vision, machine learning, chaos and fractal.</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/williamyi96" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://scholar.google.com/citations?user=r08j39wAAAAJ" target="_blank" title="Scholar">
                      
                        <i class="fa fa-fw fa-google"></i>Scholar</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Basic-Introduction"><span class="nav-number">1.</span> <span class="nav-text">Basic Introduction</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#MDPs"><span class="nav-number">2.</span> <span class="nav-text">MDPs</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Value-Function"><span class="nav-number">3.</span> <span class="nav-text">Value Function</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Action-Value-function"><span class="nav-number">3.1.</span> <span class="nav-text">Action-Value function</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Optimal-Value-function"><span class="nav-number">3.2.</span> <span class="nav-text">Optimal Value function</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Iteration-based-on-Bellman-Equation"><span class="nav-number">4.</span> <span class="nav-text">Iteration based on Bellman Equation:</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Policy-Iteration"><span class="nav-number">4.1.</span> <span class="nav-text">Policy Iteration</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Value-Iteration"><span class="nav-number">4.2.</span> <span class="nav-text">Value Iteration</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Diff-between-policy-iteration-and-value-iteration"><span class="nav-number">4.3.</span> <span class="nav-text">Diff. between policy iteration and value iteration</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Q-Learning"><span class="nav-number">5.</span> <span class="nav-text">Q-Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Exploration-and-Exploitation"><span class="nav-number">5.1.</span> <span class="nav-text">Exploration and Exploitation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Generate-action-by-policy"><span class="nav-number">5.1.1.</span> <span class="nav-text">Generate action by policy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ita-greedy-policy"><span class="nav-number">5.1.2.</span> <span class="nav-text">ita-greedy policy</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#DQN"><span class="nav-number">6.</span> <span class="nav-text">DQN</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Dimemsion-Disaster"><span class="nav-number">6.1.</span> <span class="nav-text">Dimemsion Disaster</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Value-Function-Approximation"><span class="nav-number">6.2.</span> <span class="nav-text">Value Function Approximation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Dimensionality-Reduction"><span class="nav-number">6.3.</span> <span class="nav-text">Dimensionality Reduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Training-Q-Network-by-DQN"><span class="nav-number">6.4.</span> <span class="nav-text">Training Q-Network by DQN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Naive-DQN"><span class="nav-number">6.5.</span> <span class="nav-text">Naive DQN</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Improved-DQN"><span class="nav-number">7.</span> <span class="nav-text">Improved DQN</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Nature-DQN"><span class="nav-number">7.1.</span> <span class="nav-text">Nature DQN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reference"><span class="nav-number">7.2.</span> <span class="nav-text">Reference</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Kai Yi</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
